{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classifying fruits and vegetables using CNNs\n\nWe will be building a model to classify images of fruits and vegetables from the fruits360 dataset on kaggle.\nThere are 131 different classes of fruits and vegetables in this dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\nfrom torch import nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"project_name='fruits-360-cnn'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will be using the fruits 360 dataset from kaggle containing images of 131 different fruits and vegetables.You can find it <a href=\"https://www.kaggle.com/moltean/fruits\">here</a>\n<br>\nFirst let us check out the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/fruits/fruits-360'\n\nprint(os.listdir(data_dir)) #folders in the dataset folder\nclasses = os.listdir(data_dir + \"/Training\")\n#print first 10 classes\nprint(classes[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look inside a couple of folders, one from the training set and another from the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pineapple_files = os.listdir(data_dir + \"/Training/Pineapple\")\nprint('No. of training images for pineapples:', len(pineapple_files))\nprint(pineapple_files[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_test_files = os.listdir(data_dir + \"/Test/Fig\")\nprint('No. of test images for figs:', len(fig_test_files))\nprint(fig_test_files[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now see how many images are there for each class in the training data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a dictionary to hold the count\ntrain_image_count = {}\nfor item in classes:\n    train_image_count[item] = 0\ntrain_image_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let us find the count\nfor item in classes:\n    train_image_count[item] = len( os.listdir(data_dir + \"/Training/\"+item))\n    #print(train_image_count[item])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot the frequency distribution of the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(list(train_image_count.keys()), train_image_count.values(), color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see some class of images have more training data but mostly it is even.Because of this our model may be better at predicting these certain classes than the others","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let us load the dataset and convert them into pytorch Tensors. We can use the ImageFolder class from torchvision to load the data as PyTorch tensors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ImageFolder(data_dir+'/Training', transform=ToTensor())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us take a look at a sample element from the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = dataset[0]\nprint(img.shape, label)\nimg,dataset.classes[label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tensor is of shape 3x100x100 which means that the images are colored(has 3 channels) and are of resolution 100x100","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us take a look at some of the images from the dataset using matplotlib.First we can define a helper function to help\nus view images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_example(*dataset[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_example(*dataset[50000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe the images are pretty clear and we can easily identify the fruit by looking at the images","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Seperating dataset into training and validation sets\nBefore moving further,let us split our dataset into training and validation dataset.Creating a proper validation set is very important to measure the performance of our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use a fixed seed value to make sure we get the same validation set every time we create the model.This helps in\nevaluating different model architectures against the same validation set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 50\ntorch.manual_seed(random_seed);\nlen(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_percent = 0.05 #we take 5% data for validation at first\nval_size = int(val_percent*len(dataset))\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The jovian library can be used to store our python notebooks in an easy and organized manner at <a href=\"https://jovian.ml/\">jovian.ml</a><br>\nThe jovian library also provides a simple API for recording important parameters related to the dataset, model training, results etc. for easy reference and comparison between multiple experiments. Let's save out work to jovian and also record dataset_name, val_percent and rand_seed using jovian.log_dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import jovian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project=project_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can find your API key at your jovian.ml profile","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_dataset(dataset_name='fruits-360', val_percent=val_percent, random_seed=random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now create dataloaders for our training and validation datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size=128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us take a look at a batch of images from the training loader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_batch(train_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at valdation loader\nshow_batch(val_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for images,_ in train_dl:\n    print(images.shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the model\n\nWe wiil create a **CNN** based model to tackle this classification problem as CNNs usually work great for image datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fruits360CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 50 x 50\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 25 x 25\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),#output :256*25*25\n            nn.MaxPool2d(5, 5), # output: 256 x 5 x 5\n\n            nn.Flatten(), \n            nn.Linear(256*5*5, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 131))\n            \n        \n    def forward(self, xb):\n        return self.network(xb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Fruits360CnnModel()\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's verify that the model produces the expected output on a batch of training data. The 131 outputs for each image can be interpreted as probabilities for the 131 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (get_default_device & to_device) and a helper class DeviceDataLoader to move our model & data to the GPU as required.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now wrap our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU (if available), and use to_device to move our model to the GPU (if available).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model\nNow let us define a general fir function to train our model and also a function to evaluate our model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = to_device(Fruits360CnnModel(), device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(model, val_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The initial accuracy is around 1%, which is what one might expect from a randomly intialized model (since it has a 1 in 131 chance of getting a label right by guessing randomly).\n\nWe'll use the following hyperparmeters (learning rate, no. of epochs, batch_size etc.) to train our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 3\nopt_func = torch.optim.Adam\nlr = 0.001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's important to record the hyperparameters of every experiment you do, to replicate it later and compare it against other experiments. We can record them using jovian.log_hyperparams.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.reset()\njovian.log_hyperparams({\n    'num_epochs': num_epochs,\n    'opt_func': opt_func.__name__,\n    'batch_size': batch_size,\n    'lr': lr,\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us begin training by training the model for the first 3 epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as we have recorded the hyperparameters, we can also record the final metrics achieved by the model using jovian.log_metrics for reference, analysis and comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_metrics(train_loss=history[-1]['train_loss'], \n                   val_loss=history[-1]['val_loss'], \n                   val_acc=history[-1]['val_acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also plot the valdation set accuracies to study how the model improves over time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracies(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also plot the training and validation losses to study the trend","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before continuing, let us save our work to the cloud using jovian.commit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project=project_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing images from Test data set\n\n### Testing individual images\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset. We begin by creating a test dataset using the ImageFolder class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = ImageFolder(data_dir+'/Test', transform=ToTensor())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define a helper function predict_image, which returns the predicted label for a single image tensor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = test_dataset[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = test_dataset[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, label = test_dataset[6157]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n\nAs a final step, let's also look at the overall loss and accuracy of the model on the test set, and record using jovian. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we got only 91% accuracy on the test data while we got 98% on the validation data.This might be due to the validation set not having a sufficient distribution of images.We should be able to improve the accuracy on the test data if we increase the size of the validation set and train for a few more epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving and Loading the Model\nSince we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'fruits360-cnn.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The .state_dict method returns an OrderedDict containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the .load_state_dict method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = to_device(Fruits360CnnModel(), device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.load_state_dict(torch.load('fruits360-cnn.pth'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as a sanity check, let's verify that this model has the same loss and accuracy on the test set as before","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(model2, test_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets make one final commit to jovian","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit(project=project_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}